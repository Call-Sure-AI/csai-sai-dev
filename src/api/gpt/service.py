from openai import AsyncOpenAI
from typing import AsyncGenerator, List
from src.config.settings import GPT_API_KEY, DEFAULT_GPT_MODEL, DEFAULT_EMBEDDING_MODEL
from src.api.gpt.utils import (
    validate_gpt_message_format,
    construct_gpt_developer_message,
    construct_gpt_user_message,
    construct_gpt_assistant_message,
    log_and_upload_to_s3,
    split_text_into_chunks,
)
import logging

# Initialize OpenAI client
async_client = AsyncOpenAI(api_key=GPT_API_KEY)


async def stream_text_response(messages: List[dict], model: str = DEFAULT_GPT_MODEL, temperature: float = 0.7) -> AsyncGenerator[str, None]:
    """
    Stream a conversational text response from GPT in real-time.

    Args:
        messages (List[dict]): The conversation history consisting of message objects.
            Each object should have a "role" (e.g., "system", "user", or "assistant")
            and "content" (the message text).
            Example:
                [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "What is the weather today?"}
                ]
        model (str): The GPT model to use for text generation. Defaults to the value
            specified in DEFAULT_GPT_MODEL (set in settings.py).
        temperature (float): A value between 0 and 1 that controls the randomness of
            the response. Higher values produce more creative responses, while lower
            values make the output more deterministic.

    Yields:
        str: A chunk of the response text streamed in real-time from GPT.

    Raises:
        RuntimeError: If an error occurs during the streaming process, logs the error
        and raises it.

    Example:
        async for chunk in stream_text_response(messages):
            print(chunk)
    """
    if not validate_gpt_message_format(messages):
        raise ValueError("Invalid GPT message format.")

    try:
        stream = await async_client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            stream=True  # Enable streaming
        )

        async for chunk in stream:
            if "choices" in chunk and chunk.choices[0].delta.content is not None:
                yield chunk.choices[0].delta.content
    except Exception as e:
        logging.error(f"Error during GPT streaming: {e}")
        raise RuntimeError(f"Error during GPT streaming: {e}") from e


async def generate_full_response(messages: List[dict], model: str = DEFAULT_GPT_MODEL, temperature: float = 0.7, max_tokens: int = 150) -> str:
    """
    Generate a complete conversational response from GPT in one API call.

    Args:
        messages (List[dict]): The conversation history consisting of message objects.
            Each object should have a "role" (e.g., "system", "user", or "assistant")
            and "content" (the message text).
            Example:
                [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Explain the warranty policy."}
                ]
        model (str): The GPT model to use for text generation. Defaults to the value
            specified in DEFAULT_GPT_MODEL (set in settings.py).
        temperature (float): A value between 0 and 1 that controls the randomness of
            the response. Higher values produce more creative responses, while lower
            values make the output more deterministic.
        max_tokens (int): The maximum number of tokens to generate in the response.
            Defaults to 150.

    Returns:
        str: The full response text generated by GPT.

    Raises:
        RuntimeError: If an error occurs during the response generation process, logs
        the error and raises it.

    Example:
        response = await generate_full_response(messages)
        print(response)
    """
    if not validate_gpt_message_format(messages):
        raise ValueError("Invalid GPT message format.")

    try:
        response = await async_client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message["content"].strip()
    except Exception as e:
        logging.error(f"Error generating full GPT response: {e}")
        raise RuntimeError(f"Error generating full GPT response: {e}") from e


# ================================= Embeddings API =================================
async def generate_embedding(input_text: str, model: str = DEFAULT_EMBEDDING_MODEL) -> List[float]:
    """
    Generate an embedding vector for a given input text using OpenAI's Embedding API.

    Args:
        input_text (str): The text to generate the embedding for. The text should be a
            meaningful sentence or paragraph, as the embedding captures semantic
            relationships.
        model (str): The embedding model to use. Defaults to the value specified in
            DEFAULT_EMBEDDING_MODEL (set in settings.py).

    Returns:
        List[float]: A list of floating-point numbers representing the embedding vector.

    Raises:
        RuntimeError: If an error occurs during the embedding generation process, logs
        the error and raises it.

    Example:
        embedding = await generate_embedding("What is the return policy?")
        print(embedding)
    """
    try:
        response = await async_client.embeddings.create(
            input=input_text,
            model=model
        )
        return response.data[0].embedding
    except Exception as e:
        logging.error("Error generating embedding: %s", str(e))
        raise RuntimeError(f"Error generating embedding: {e}") from e


async def batch_generate_embeddings(input_texts: List[str], model: str = DEFAULT_EMBEDDING_MODEL) -> List[List[float]]:
    """
    Generate embeddings for a batch of input texts using OpenAI's Embedding API.

    Args:
        input_texts (List[str]): A list of text strings for which to generate embeddings.
            Each text should represent meaningful information to ensure semantic
            embeddings.
        model (str): The embedding model to use. Defaults to the value specified in
            DEFAULT_EMBEDDING_MODEL (set in settings.py).

    Returns:
        List[List[float]]: A list of embedding vectors. Each vector corresponds to a
        text in the input list.

    Raises:
        RuntimeError: If an error occurs during the batch embedding generation process,
        logs the error and raises it.

    Example:
        texts = [
            "What is the return policy?",
            "How long does the warranty last?"
        ]
        embeddings = await batch_generate_embeddings(texts)
        print(embeddings)
    """
    try:
        response = await async_client.embeddings.create(
            input=input_texts,
            model=model
        )
        return [item.embedding for item in response.data]
    except Exception as e:
        logging.error("Error generating embeddings for batch: %s", str(e))
        raise RuntimeError(f"Error generating embeddings for batch: {e}") from e
